{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6247033b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "executionInfo": {
     "elapsed": 5305,
     "status": "error",
     "timestamp": 1688128208706,
     "user": {
      "displayName": "Joyesh Meshram",
      "userId": "08771931488029831946"
     },
     "user_tz": -330
    },
    "id": "6247033b",
    "outputId": "ec252733-4541-4566-f349-87e52d262a56",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time  # For handling time-related functions\n",
    "import re  # For regular expressions\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "from bs4 import BeautifulSoup  # For web scraping\n",
    "from selenium import webdriver  # For browser automation\n",
    "from selenium.webdriver.chrome.service import Service  # For configuring the ChromeDriver service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de384384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_driver():\n",
    "    \"\"\"\n",
    "    Creates and configures a Chrome WebDriver instance for web scraping.\n",
    "\n",
    "    Returns:\n",
    "        WebDriver: Configured instance of the Chrome WebDriver.\n",
    "    \"\"\"\n",
    "    # Set the path to the ChromeDriver executable\n",
    "    service = Service('D:/chromedriver-win64/chromedriver.exe')\n",
    "\n",
    "    # Configure Chrome options\n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "    # Ignore certificate errors\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    \n",
    "    # Start the browser in maximized mode\n",
    "    options.add_argument('--start-maximized')\n",
    "\n",
    "    # Create a Chrome WebDriver instance with the specified service and options\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    # Return the WebDriver instance\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e498a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_page_source(url, delay=10):\n",
    "    \"\"\"\n",
    "    Retrieves the page source of a specified URL using Selenium and BeautifulSoup.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to scrape.\n",
    "        delay (int, optional): Delay in seconds to wait for the page to load. Default is 10 seconds.\n",
    "\n",
    "    Returns:\n",
    "        BeautifulSoup: The BeautifulSoup object representing the page source.\n",
    "    \"\"\"\n",
    "    # Get a Chrome WebDriver instance\n",
    "    driver = get_driver()\n",
    "\n",
    "    # Open the specified URL in the browser\n",
    "    driver.get(url)\n",
    "\n",
    "    # Allow time for the page to load (adjust delay as needed)\n",
    "    time.sleep(delay)\n",
    "\n",
    "    # Get the page source using BeautifulSoup for parsing\n",
    "    page_source = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Close the WebDriver to release resources\n",
    "    driver.quit()\n",
    "\n",
    "    # Return the parsed page source\n",
    "    return page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5b7b4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_titles(page_source, first_page=False):\n",
    "    \"\"\"\n",
    "    Extracts titles of problems from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "        first_page (bool, optional): Flag indicating whether it's the first page. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        list: List of extracted titles.\n",
    "    \"\"\"\n",
    "    # Determine the starting index based on whether it's the first page or not\n",
    "    start_index = 1 if first_page else 0\n",
    "\n",
    "    # Find all title elements using BeautifulSoup\n",
    "    title_elements = page_source.find_all(\n",
    "        'a',\n",
    "        class_=[\n",
    "            'h-5 hover:text-blue-s dark:hover:text-dark-blue-s',\n",
    "            'h-5 hover:text-blue-s dark:hover:text-dark-blue-s opacity-60'\n",
    "        ]\n",
    "    )[start_index:]\n",
    "\n",
    "    # Extract text from title elements and store in a list\n",
    "    titles = [title_element.text for title_element in title_elements]\n",
    "\n",
    "    # Return the list of titles\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4775d65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_problems_URL(page_source, first_page=False):\n",
    "    \"\"\"\n",
    "    Extracts URLs of problems from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "        first_page (bool, optional): Flag indicating whether it's the first page. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        list: List of extracted problem URLs.\n",
    "    \"\"\"\n",
    "    # Determine the starting index based on whether it's the first page or not\n",
    "    start_index = 1 if first_page else 0\n",
    "\n",
    "    # Find all problem elements with an 'a' tag and an 'href' attribute using BeautifulSoup\n",
    "    problem_elements = page_source.find_all('a', href=True, class_=[\n",
    "        'h-5 hover:text-blue-s dark:hover:text-dark-blue-s',\n",
    "        'h-5 hover:text-blue-s dark:hover:text-dark-blue-s opacity-60'\n",
    "    ])[start_index:]\n",
    "\n",
    "    # Extract the 'href' attribute values from the problem elements and store in a list\n",
    "    problems_url = [el['href'] for el in problem_elements]\n",
    "\n",
    "    # Return the list of problem URLs\n",
    "    return problems_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08095d92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_acceptances_difficulties(page_source, first_page=False):\n",
    "    \"\"\"\n",
    "    Extracts acceptances and difficulties of problems from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "        first_page (bool, optional): Flag indicating whether it's the first page. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two lists - acceptances and difficulties.\n",
    "    \"\"\"\n",
    "    # Find all div elements with the specified class using BeautifulSoup\n",
    "    div_elements = page_source.find_all('div', class_='mx-2 flex items-center py-[11px]')\n",
    "\n",
    "    # Determine the starting index based on whether it's the first page or not\n",
    "    start_index = 1 if first_page else 0\n",
    "\n",
    "    # Extract text from the span elements within the div elements and store in a list\n",
    "    items = [\n",
    "        span_element.text.strip()\n",
    "        for div_element in div_elements\n",
    "        for span_element in [div_element.find('span')]\n",
    "        if span_element\n",
    "    ]\n",
    "\n",
    "    # Separate the items into acceptances and difficulties lists\n",
    "    acceptances, difficulties = [], []\n",
    "    for item in items:\n",
    "        if item:\n",
    "            (acceptances if item.endswith('%') else difficulties).append(item)\n",
    "\n",
    "    # Adjust lists based on the starting index\n",
    "    acceptances = acceptances[start_index:]\n",
    "    difficulties = difficulties[start_index:]\n",
    "\n",
    "    # Return the lists of acceptances and difficulties\n",
    "    return acceptances, difficulties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bf93e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_single_page_df(url, first_page=False):\n",
    "    \"\"\"\n",
    "    Creates a Pandas DataFrame for a single page of problems.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the page to scrape.\n",
    "        first_page (bool, optional): Flag indicating whether it's the first page. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame containing titles, problem URLs, acceptances, and difficulties.\n",
    "    \"\"\"\n",
    "    # Get the page source for the specified URL\n",
    "    page_source = get_page_source(url)\n",
    "\n",
    "    # Extract titles, problem URLs, acceptances, and difficulties from the page source\n",
    "    titles = get_titles(page_source, first_page)\n",
    "    problems_url = get_problems_URL(page_source, first_page)\n",
    "    acceptances, difficulties = get_acceptances_difficulties(page_source, first_page)\n",
    "\n",
    "    # Create a dictionary with the extracted data\n",
    "    data = {\n",
    "        'title': titles,\n",
    "        'problem_URL': problems_url,\n",
    "        'acceptance': acceptances,\n",
    "        'difficulty': difficulties\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame using the dictionary\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Return the DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22b3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_multiple_page_df(start=1, end=60):\n",
    "    \"\"\"\n",
    "    Calls get_single_page_df for a range of pages and concatenates the results into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        start (int, optional): The starting page. Default is 1.\n",
    "        end (int, optional): The ending page. Default is 60.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame containing titles, problem URLs, acceptances, and difficulties for multiple pages.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store DataFrames for each page\n",
    "    list_of_dfs = []\n",
    "\n",
    "    # Set the flag for the first page\n",
    "    first_page = True if start == 1 else False\n",
    "\n",
    "    # Iterate over the specified range of pages\n",
    "    for i in range(start, end + 1):\n",
    "        # Construct the URL for the current page\n",
    "        url = 'https://leetcode.com/problemset/all/?page=' + str(i)\n",
    "\n",
    "        # Get the DataFrame for the current page and append it to the list\n",
    "        df = get_single_page_df(url, first_page)\n",
    "        list_of_dfs.append(df)\n",
    "\n",
    "        # Update the first_page flag for subsequent pages\n",
    "        first_page = False\n",
    "\n",
    "    # Concatenate the list of DataFrames into a single DataFrame\n",
    "    df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "\n",
    "    # Return the final DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6c4610",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape(start=1, end=60, file_name='part1.csv'):\n",
    "    \"\"\"\n",
    "    Initiates the scraping process, saving the resulting DataFrame to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        start (int, optional): The starting page for scraping. Default is 1.\n",
    "        end (int, optional): The ending page for scraping. Default is 60.\n",
    "        file_name (str, optional): The name of the CSV file to save the scraped data. Default is 'part1.csv'.\n",
    "    \"\"\"\n",
    "    # Get the DataFrame by scraping multiple pages\n",
    "    df = get_multiple_page_df(start, end)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(path_or_buf=file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d21c0f",
   "metadata": {},
   "source": [
    "`Running Web-Scraping Process`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a4797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scrape(start=1, end=60, file_name='x1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b74fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('x1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff93e2d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1['problem_URL'] = df1['problem_URL'].apply(lambda x: f'{\"https://leetcode.com\"}{x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74868a4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1['solution_URL'] = df1['problem_URL'].apply(lambda x: f'{x}{\"/solution\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e60fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_title(page_source):\n",
    "    \"\"\"\n",
    "    Extracts the title from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted title text.\n",
    "    \"\"\"\n",
    "    # Find the title element using BeautifulSoup\n",
    "    title_element = page_source.find(\n",
    "        'a',\n",
    "        class_='mr-2 text-label-1 dark:text-dark-label-1 hover:text-label-1 dark:hover:text-dark-label-1 text-lg font-medium'\n",
    "    )\n",
    "\n",
    "    # Extract the text content from the title element\n",
    "    title = title_element.text\n",
    "\n",
    "    # Return the title\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df08e106",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_problem_description(page_source):\n",
    "    \"\"\"\n",
    "    Extracts the problem description from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted problem description text.\n",
    "    \"\"\"\n",
    "    # Find the div element containing the problem description using BeautifulSoup\n",
    "    description_element = page_source.find(\n",
    "        'div',\n",
    "        class_='xFUwe'\n",
    "    )\n",
    "\n",
    "    # Extract the text content from the description element\n",
    "    description_text = description_element.text\n",
    "\n",
    "    # Return the problem description\n",
    "    return description_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a38d99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_topic_tags(page_source):\n",
    "    \"\"\"\n",
    "    Extracts the topic tags from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        str: Comma-separated string of extracted topic tags.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store topic tags\n",
    "    topic_tags = []\n",
    "\n",
    "    # Find all elements with the specified class using BeautifulSoup\n",
    "    topic_tag_elements = page_source.find_all('a',\n",
    "                 class_='mr-4 rounded-xl px-2 py-1 text-xs transition-colors text-label-2 dark:text-dark-label-2 hover:text-label-2 dark:hover:text-dark-label-2 bg-fill-3 dark:bg-dark-fill-3 hover:bg-fill-2 dark:hover:bg-dark-fill-2') \n",
    "\n",
    "    # Extract text content from each topic tag element and append to the list\n",
    "    for topic_tag_element in topic_tag_elements:\n",
    "        topic_tag = topic_tag_element.text\n",
    "        topic_tags.append(topic_tag)\n",
    "\n",
    "    # Join the list of topic tags into a comma-separated string\n",
    "    topic_tags_str = ', '.join(f\"'{item}'\" for item in topic_tags)\n",
    "\n",
    "    # Return the formatted string of topic tags\n",
    "    return topic_tags_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c719c504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_accepted(page_source):\n",
    "    \"\"\"\n",
    "    Extracts the number of accepted submissions from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        Tag: The BeautifulSoup Tag object containing the information about accepted submissions.\n",
    "    \"\"\"\n",
    "    # Find all elements with the specified class using BeautifulSoup\n",
    "    accepted_elements = page_source.find_all('div', class_='text-label-1 dark:text-dark-label-1 text-sm font-medium')\n",
    "\n",
    "    # Return the first element (it contains the information about accepted submissions)\n",
    "    return accepted_elements[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f47ffdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_submission(page_source):\n",
    "    \"\"\"\n",
    "    Extracts the number of total submissions from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        Tag: The BeautifulSoup Tag object containing the information about total submissions.\n",
    "    \"\"\"\n",
    "    # Find all elements with the specified class using BeautifulSoup\n",
    "    submission_elements = page_source.find_all('div', class_='text-label-1 dark:text-dark-label-1 text-sm font-medium')\n",
    "\n",
    "    # Return the second element (it contains the information about total submissions)\n",
    "    return submission_elements[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a90d6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_solution(page_source):\n",
    "    \"\"\"\n",
    "    Extracts the solution count from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted solution.\n",
    "    \"\"\"\n",
    "    # Find the element with a link containing 'solutions' using BeautifulSoup\n",
    "    solutions_element = page_source.find(href=re.compile(\"solutions\"))\n",
    "\n",
    "    # Use regular expression to extract the solution type from the text\n",
    "    solution = re.findall(r\"\\((.*?)\\)\", solutions_element.text)[0]\n",
    "\n",
    "    # Return the extracted solution count\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fda21e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_discussion_count(page_source):\n",
    "    \"\"\"\n",
    "    Extracts the count of discussions from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted count of discussions.\n",
    "    \"\"\"\n",
    "    # Find the element with the specified class using BeautifulSoup\n",
    "    discussion_count_element = page_source.find_all('div', class_='flex-1 text-sm leading-[22px]')[0]\n",
    "\n",
    "    # Use regular expression to extract the count of discussions from the element's text\n",
    "    discussion_count = re.findall(r\"\\((.*?)\\)\", discussion_count_element.text)[0]\n",
    "\n",
    "    # Return the extracted count of discussions\n",
    "    return discussion_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a4232b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_likes(page_source):\n",
    "    \"\"\"\n",
    "    Extracts the number of likes from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted number of likes.\n",
    "    \"\"\"\n",
    "    # Find the element with the specified class using BeautifulSoup\n",
    "    likes_element = page_source.find_all('div', class_='text-lg text-gray-6 dark:text-dark-gray-6')[0]\n",
    "\n",
    "    # Find the next sibling element and extract the text content\n",
    "    likes_count = likes_element.find_next_sibling().text\n",
    "\n",
    "    # Return the extracted number of likes\n",
    "    return likes_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e17a9d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_dislikes(page_source):\n",
    "    \"\"\"\n",
    "    Extracts the number of dislikes from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted number of dislikes.\n",
    "    \"\"\"\n",
    "    # Find the element with the specified class using BeautifulSoup\n",
    "    dislikes_element = page_source.find_all('div', class_='text-lg text-gray-6 dark:text-dark-gray-6')[1]\n",
    "\n",
    "    # Find the next sibling element and extract the text content\n",
    "    dislikes_count = dislikes_element.find_next_sibling().text\n",
    "\n",
    "    # Return the extracted number of dislikes\n",
    "    return dislikes_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aaea3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_similar_questions(page_source):\n",
    "    \"\"\"\n",
    "    Extracts a list of similar questions from the given page source.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        str: Comma-separated string of extracted similar questions.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store similar questions\n",
    "    similar_questions = []\n",
    "\n",
    "    # Find all elements with the specified class using BeautifulSoup\n",
    "    similar_question_elements = page_source.find_all('a', class_='text-sm font-medium transition-none text-label-1 dark:text-dark-label-1 hover:text-blue-s dark:hover:text-dark-blue-s')\n",
    "\n",
    "    # Extract text content from each similar question element and append to the list\n",
    "    for similar_question_element in similar_question_elements:\n",
    "        similar_question = similar_question_element.text\n",
    "        similar_questions.append(similar_question)\n",
    "\n",
    "    # Join the list of similar questions into a comma-separated string\n",
    "    similar_questions_str = ', '.join(f\"'{item}'\" for item in similar_questions)\n",
    "\n",
    "    # Return the formatted string of similar questions\n",
    "    return similar_questions_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccecf835",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_page_source(driver, url, delay=10):\n",
    "    \"\"\"\n",
    "    Retrieves the page source of a specified URL using Selenium and BeautifulSoup.\n",
    "\n",
    "    Args:\n",
    "        driver (webdriver): The Selenium WebDriver object.\n",
    "        url (str): The URL to scrape.\n",
    "        delay (int, optional): Delay in seconds to wait for the page to load. Default is 10 seconds.\n",
    "\n",
    "    Returns:\n",
    "        BeautifulSoup: The BeautifulSoup object representing the page source.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open the specified URL in the browser\n",
    "    driver.get(url)\n",
    "\n",
    "    # Allow time for the page to load (adjust delay as needed)\n",
    "    time.sleep(delay)\n",
    "\n",
    "    # Get the page source using BeautifulSoup for parsing\n",
    "    page_source = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Return the parsed page source\n",
    "    return page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2ebcbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_is_premium(page_source):\n",
    "    \"\"\"\n",
    "    Checks if the page indicates a premium status.\n",
    "\n",
    "    Args:\n",
    "        page_source (BeautifulSoup): The BeautifulSoup object representing the page source.\n",
    "\n",
    "    Returns:\n",
    "        str: 'True' if premium, 'False' otherwise.\n",
    "    \"\"\"\n",
    "    # Find the element with the specified class using BeautifulSoup\n",
    "    premium_element = page_source.find('div', class_='text-md mb-6 text-center text-label-2 dark:text-dark-label-2')\n",
    "\n",
    "    # Determine premium status based on the existence of the element\n",
    "    is_premium = 'True' if premium_element else 'False'\n",
    "\n",
    "    # Return the premium status\n",
    "    return is_premium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9fcb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape(df1, start=1, end=3000, file_name='x2.csv'):\n",
    "    \"\"\"\n",
    "    Scrapes data for a range of links from the provided DataFrame and saves it to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        df1 (DataFrame): The DataFrame containing problem URLs.\n",
    "        start (int, optional): The starting index for scraping. Default is 1.\n",
    "        end (int, optional): The ending index for scraping. Default is 3000.\n",
    "        file_name (str, optional): The name of the CSV file to save the scraped data. Default is 'x2.csv'.\n",
    "    \"\"\"\n",
    "    # Extract links for the specified range from the DataFrame\n",
    "    links = df1['problem_URL'][start - 1:end]\n",
    "\n",
    "    # Initialize an empty list to store DataFrames for each link\n",
    "    dfs = []\n",
    "    \n",
    "    # Set the path to the ChromeDriver executable\n",
    "    service = Service('D:/chromedriver-win64/chromedriver.exe')\n",
    "\n",
    "    # Configure Chrome options\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--ignore-certificate-errors')  # Ignore certificate errors\n",
    "    options.add_argument('--start-maximized')  # Start the browser in maximized mode\n",
    "\n",
    "    # Create a Chrome WebDriver instance with the specified service and options\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    # Iterate over the links and scrape data\n",
    "    for link in links:\n",
    "        i = 0\n",
    "        # Get the page source for the current link\n",
    "        page_source = get_page_source(driver, link, delay=10)\n",
    "\n",
    "        # Create a dictionary to store scraped data\n",
    "        data = {'is_premium': get_is_premium(page_source)}\n",
    "\n",
    "        # Check if the problem is not premium before scraping additional data\n",
    "        if data['is_premium'] == 'False':\n",
    "            # Update the data dictionary with additional scraped data\n",
    "            data.update({\n",
    "                'title': get_title(page_source),\n",
    "                'problem_description': get_problem_description(page_source),\n",
    "                'topic_tags': get_topic_tags(page_source),\n",
    "                'accepted': get_accepted(page_source),\n",
    "                'submission': get_submission(page_source),\n",
    "                'solution': get_solution(page_source),\n",
    "                'discussion_count': get_discussion_count(page_source),\n",
    "                'likes': get_likes(page_source),\n",
    "                'dislikes': get_dislikes(page_source),\n",
    "                'similar_questions': get_similar_questions(page_source)\n",
    "            })\n",
    "\n",
    "            # Create a DataFrame for the current link and append it to the list\n",
    "            df = pd.DataFrame(data, index=[i])\n",
    "            dfs.append(df)\n",
    "            i += 1\n",
    "    print(dfs)\n",
    "    # Concatenate the list of DataFrames into a single DataFrame\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Save the final DataFrame to a CSV file\n",
    "    df.to_csv(path_or_buf=file_name, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084a3d77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scrape(df1, start=1, end=3000, file_name='x2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d300eea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('x2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03de23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df1.merge(df2, left_on='title', right_on='title', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe0a22c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('leetcode_scraped_data.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
